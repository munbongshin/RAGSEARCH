from dotenv import load_dotenv, set_key
from pathlib import Path
import streamlit as st
import sys, platform
import asyncio
import re
from langchain_core.messages import ChatMessage
from langchain_core.documents import Document
#from langchain.llms import Ollama
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import os, base64, time
import subprocess, requests
from langchain_community.llms import Ollama
from langchain_groq import ChatGroq  #pip install langchain-groq
from langchain_openai import ChatOpenAI
from backend.app.TextSummarizer import TextSummarizer #by mbs
from backend.app.GroqManager import  GroqManager
from backend.app.GroqManager import get_groq_response as GroqResponse
from backend.app.ChromaDbManager import ChromaDbManager
from backend.app.CustomSentenceTransformerEmbeddings import CustomSentenceTransformerEmbeddings as CSTFM

import logging, json


logging.basicConfig(level=logging.ERROR)
logging.getLogger("langchain").setLevel(logging.ERROR)
logger = logging.getLogger(__name__)


st.set_page_config(page_title="AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°", layout="wide")
st.cache_data.clear()

FILLTERED_DOC_NUMBER = 5
DOCKER_USE = True
SIMILALITY = 0.9
#@st.cache_data
def load_embeddings():
    return CSTFM()

@st.cache_resource
def load_llm():
    project_root = Path(__file__).parent.parent
    env_path = project_root / '.env'
    load_dotenv(dotenv_path=env_path)
    
    baseurl = os.environ.get("BASE_URL")
    modelllm = os.environ.get("DEFAULT_LLMNAME")
    api_key = os.environ.get("API_KEY", "lm-studio")  # API í‚¤ë„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´

    if not all([baseurl, modelllm]):
        raise ValueError("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    chatllm = ChatOpenAI(
        base_url=baseurl,
        api_key=api_key,        
        model=modelllm,
        streaming=True,
        temperature=0,
    )
    
    return chatllm

class RAGChatAppUI:
    def __init__(self, app):
        try:
            self.app = app
            self.llm_name = "LM Studio"
            os.environ['STREAMLIT_CACHE_STORAGE_MANAGER'] = 'FileStorageManager'
            if 'messages' not in st.session_state:
                st.session_state.messages = []
            if 'search_results' not in st.session_state:
                st.session_state.search_results = []
            if 'chat_input_key' not in st.session_state:
                st.session_state.chat_input_key = f"chat_input_query_{int(time.time() * 1000)}"
            
            
        except Exception as e:
            error_message = f"_init_ ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def setup_main_ui(self):
        try:
            st.header("Ask your PDF, DOC, PPT, Excel, HWP ğŸ’¬")
            
            main_content = st.empty()
            with main_content.container():
                self.maincol1, self.maincol2 = st.columns([7, 3])
        except Exception as e:
            error_message = f"set_main_ui ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            
    def set_llm_name(self, llm_name):
        self.llm_name =  llm_name

    def setup_sidebar(self):
        try:
            with st.sidebar:
                st.header("Rag Menu")
                tab1, tab2, tab3, tab4 = st.tabs(["Arg Chat", "ì„ë² ë”©", "ChromaDB ê´€ë¦¬", "LLM ëª¨ë¸"])
                
                with tab1:
                    self.update_tab("Arg Chat")
                    self.setup_rag_chat_tab()
                with tab2:
                    self.update_tab("ì„ë² ë”©")
                    self.setup_embed_tab()
                with tab3:
                    self.update_tab("ChromaDB ê´€ë¦¬")
                    self.setup_chromadb_tab()
                with tab4:
                    self.update_tab("LLM ëª¨ë¸")
                    self.setup_llmmodel_tab()
        except Exception as e:
            error_message = f"setup_sidebar ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
    
    def update_tab(self, tab_name):
        if st.session_state.current_tab != tab_name:
            st.session_state.current_tab = tab_name
            self.app.update_collections()

    def setup_rag_chat_tab(self):
        try:
            st.header("Arg Chat")
            st.session_state.rag_usage = st.radio("Ragì´ìš©ì—¬ë¶€ ğŸ‘‡", ["Rag+LLM", "LLM"], index=0 if st.session_state.rag_usage == "Rag+LLM" else 1)
            collections = st.session_state.updated_collections            
            selected_collection = st.selectbox("ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="search_collection_name")
            self.app.set_collection_name(selected_collection)
            self.setup_document_selection(selected_collection)
        except Exception as e:
            error_message = f"setup_arg_chat_tab ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def setup_document_selection(self, collection_name):
        try:            
            source_search = st.text_input("ë¬¸ì„œê²€ìƒ‰:", key="source_search_input")
            
            if st.button("ê²€ìƒ‰"):
                st.session_state.filtered_sources = self.app.db_manager.get_all_documents_source(collection_name, source_search)
                st.session_state.show_search_results = True
            
            if st.session_state.show_search_results:
                with st.expander("ê²€ìƒ‰ ê²°ê³¼", expanded=True):
                    st.write("ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ íƒí•˜ì„¸ìš”:")
                    for source in st.session_state.filtered_sources:
                        if st.checkbox(source, key=f"check_{source}"):
                            if source not in st.session_state.selected_sources:
                                st.session_state.selected_sources.append(source)
                        elif source in st.session_state.selected_sources:
                            st.session_state.selected_sources.remove(source)                
                    if st.button("ì„ íƒ ì™„ë£Œ"):
                        st.session_state.show_search_results = False
                        
            selected = st.multiselect(
                "ì„ íƒëœ ë¬¸ì„œ:", 
                options=st.session_state.filtered_sources,
                default=st.session_state.selected_sources,
                key="final_selected_sources",
            )
            
            st.session_state.selected_sources = selected
            
            col1, col2 = st.columns(2)
            with col1: 
                st.session_state.apply_filter = st.checkbox("ì ìš©", value=st.session_state.apply_filter)
            with col2:
                if st.button("í•„í„° ì´ˆê¸°í™”"):
                    st.session_state.selected_sources = []
                    st.session_state.filtered_sources = []
                    st.session_state.apply_filter = False
        except Exception as e:
            error_message = f"setup_document_collection ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def setup_embed_tab(self):
        try:
            st.header("ì„ë² ë”©")
            
            collections = st.session_state.updated_collections
            
            if collections:
                selected_collection = st.selectbox("ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="embed_collection_select")
            else:
                st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì»¬ë ‰ì…˜ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
                return
            # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if 'file_uploader_key' not in st.session_state:
                st.session_state.file_uploader_key = 0
            if 'uploaded_files' not in st.session_state:
                st.session_state.uploaded_files = []

            # íŒŒì¼ ì—…ë¡œë” ì´ˆê¸°í™” ë²„íŠ¼
            if st.button("íŒŒì¼ ì—…ë¡œë” ì´ˆê¸°í™”"):
                old_key = f"file_uploader_{st.session_state.file_uploader_key}"
                st.session_state.file_uploader_key += 1
                st.session_state.uploaded_files = []
                # ì´ì „ file_uploader_key ì‚­ì œ
                if old_key in st.session_state:
                    del st.session_state[old_key]

            # íŒŒì¼ ì—…ë¡œë”
            current_key = f"file_uploader_{st.session_state.file_uploader_key}"
            uploaded_files = st.file_uploader("íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", 
                                            type=['.pdf','.pptx','.ppt','.doc','.docx','.hwp','.hwpx','.xlsx','.md','.txt','html','.htm'], 
                                            accept_multiple_files=True,
                                            key=current_key)
            
            # ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
            if uploaded_files:
                st.session_state.uploaded_files = uploaded_files
           
            # ë²¡í„° ì €ì¥ ë²„íŠ¼
            if st.button("ë²¡í„°ì €ì¥", key="store_vector_collection"):            
                if st.session_state.uploaded_files:                
                    total_chunks_stored = 0
                    for file in st.session_state.uploaded_files:
                        if self.app.db_manager.check_source_exists(selected_collection, file):
                            st.info(f"{file.name} íŒŒì¼ì´ DBì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        else:                            
                            try:
                                with st.spinner(f"{file.name} íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                                    text = self.app.db_manager.extract_text_from_file(file, file.name)
                                
                                if not text:
                                    st.warning(f"{file.name}ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                with st.spinner(f"{file.name} í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘..."):
                                    processed_text = text
                                    chunks_stored = self.app.db_manager.split_embed_docs_store(
                                        processed_text, file.name, selected_collection)
                                
                                total_chunks_stored += chunks_stored
                                st.success(f"'{file.name}' íŒŒì¼ì´ ì²˜ë¦¬ë˜ì–´ {chunks_stored}ê°œì˜ ì²­í¬ê°€ '{selected_collection}' ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"{file.name} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    
                    st.success(f"ì´ {total_chunks_stored}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.session_state.uploaded_files = []  # ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                    
                    # íŒŒì¼ ì—…ë¡œë” ë¦¬ì…‹ ë° ì´ì „ í‚¤ ì‚­ì œ
                    old_key = current_key
                    st.session_state.file_uploader_key += 1
                    if old_key in st.session_state:
                        del st.session_state[old_key]
                else:
                    st.warning("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

        except Exception as e:
            error_message = f"setup_embed_tab ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)       

    def setup_chromadb_tab(self):
        try:
            st.title("Arg Searchë¥¼ ìœ„í•œ ChromaDB ê´€ë¦¬ Section")
            management_menu = st.selectbox(
                "ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”",
                ("Collection ìƒì„±", "Collection ì‚­ì œ", "Collection ë‚´ìš©ê²€ìƒ‰", "Collection ë‚´ìš©ë³´ê¸°"),
                key="tabmenu_select"
            )
            
            collections = self.app.db_manager.get_list_collections()
            
            if management_menu == "Collection ìƒì„±":
                self.create_collection_ui()
            elif management_menu == "Collection ì‚­ì œ":
                self.delete_collection_ui(collections)
            elif management_menu == "Collection ë‚´ìš©ê²€ìƒ‰":
                self.search_collection_ui(collections)
            elif management_menu == "Collection ë‚´ìš©ë³´ê¸°":
                self.view_collection_ui(collections)
        except Exception as e:
            error_message = f"Setup_embed_tab ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def create_collection_ui(self):
        try:
            st.header("ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±")
            collection_name = st.text_input("ìƒì„±í•  ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”")
                       
            
            if st.button("ìƒì„±", key="create_new_collection"):
                if re.match("^[a-zA-Z0-9_-]+$", collection_name):
                    if collection_name not in st.session_state.updated_collections:
                        result = self.app.db_manager.create_collection(collection_name)
                        if result:
                            st.session_state.need_update = True
                            st.success(f"ì»¬ë ‰ì…˜ '{collection_name}'ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                        else:
                            st.error(f"ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.info(f"ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
                    if st.session_state.need_update:
                        self.app.update_collections()
                else:
                    st.error("ì˜ë¬¸ê³¼ ìˆ«ì, -, _ë§Œ ì…ë ¥ í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            st.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                    
    def delete_collection_ui(self, collections):
        try:
            if 'delete_state' not in st.session_state:
                st.session_state.delete_state = 'initial'
            if 'selected_collection' not in st.session_state:
                st.session_state.selected_collection = None
            if 'show_message' not in st.session_state:
                st.session_state.show_message = None
            if 'confirm_delete' not in st.session_state:
                st.session_state.confirm_delete = False
            if 'updated_collections' not in st.session_state:
                st.session_state.updated_collections = self.app.db_manager.get_list_collections()

            # ë©”ì‹œì§€ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
            if st.session_state.show_message:
                st.success(st.session_state.show_message)
                st.session_state.show_message = None

            st.session_state.selected_collection = st.selectbox(
                "ì‚­ì œí•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", 
                st.session_state.updated_collections, 
                key="delete_collection_select"
            )

            if st.button("ì‚­ì œ", key="delete_collection"):
                st.session_state.delete_state = 'confirm'

            if st.session_state.delete_state == 'confirm':
                st.write(f"'{st.session_state.selected_collection}' ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ì˜ˆ", key="confirm_yes"):
                        st.session_state.confirm_delete = True
                with col2:
                    if st.button("ì•„ë‹ˆì˜¤", key="confirm_no"):
                        st.session_state.delete_state = 'initial'
                        st.session_state.selected_collection = None

            # 'ì˜ˆ' ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
            if st.session_state.confirm_delete:
                result = self.app.db_manager.delete_collection(st.session_state.selected_collection)
                if result:
                    st.session_state.show_message = f"'{st.session_state.selected_collection}' ì»¬ë ‰ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                    st.session_state.need_update = True
                    st.session_state.delete_state = 'initial'
                    st.session_state.selected_collection = None
                    st.session_state.confirm_delete = False
                    st.rerun()
                else:
                    st.error(f"'{st.session_state.selected_collection}' ì»¬ë ‰ì…˜ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            if st.session_state.need_update:
                self.app.update_collections()
                
        except Exception as e:
            error_message = f"delete_collection_ui ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def search_collection_ui(self, collections):
        try:
            st.header("ì»¬ë ‰ì…˜ ê²€ìƒ‰")
            selected_collection = st.selectbox("ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="search_collection_select")
            search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            if st.button("ê²€ìƒ‰", key="search_collectison_content"):
                results = self.app.db_manager.search_collection(selected_collection, search_query, self.app.db_manager.docnum,SIMILALITY)                
                self.display_search_results(results)  # self.app.display_search_results(results) ëŒ€ì‹ 
        except Exception as e:
            error_message = f"search_collection_ui ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)


    def view_collection_ui(self, collections):
        try:
            st.header("ì»¬ë ‰ì…˜ ë‚´ìš©ë³´ê¸°")
            selected_collection = st.selectbox("ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="view_collection_select")            
            if st.button("ë‚´ìš© ë³´ê¸°", key="view_collection_content"):
                content = self.app.db_manager.view_collection_content(selected_collection)
                st.write(f"'{selected_collection}'ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.")
                st.markdown(content)
        except Exception as e:
            error_message = f"view_collection_ui ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
    
    
    
    def setup_ollama(self, app):
        models = self.app.get_ollama_models()
        if models:
            selected_model = st.selectbox("ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:", models)
            st.write(f"ì„ íƒí•œ ëª¨ë¸: {selected_model}")
            app.set_model_name(selected_model)
            app.llmname = selected_model
            st.success(f"Ollama ëª¨ë¸ {selected_model}ì´ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ollama pull' ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•´ì£¼ì„¸ìš”.")
            app.set_llm_model(app.lm_llm, "http://localhost:11434")

    def setup_groq(self, app):
        models = app.load_groq()
        if models:
            app.lm_llm.model_name = models
            app.set_llm_model(app.lm_llm, "")
            st.success("Groq(ì™¸ë¶€API)ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.")            
        else:
            st.error("Groq(ì™¸ë¶€API)ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì˜ GROQ_API_KEYì— keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        

    def setup_lm_studio(self, app):
        models = self.app.get_lm_studio_models()
        if models:
            app.lm_llm.model_name = models
            app.set_llm_model(app.lm_llm, "http://localhost:1234/v1")
            st.success("LM Studioì—ì„œ ë™ì¼í•œ ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì–´ì•¼ ì •ìƒì ìœ¼ë¡œ ì‘ë™ë©ë‹ˆë‹¤.")
        else:
            app.set_llm_model(app.lm_llm, "http://localhost:1234/v1")
            st.error("LM Studio ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    

    def setup_llmmodel_tab(self):
        try:
            if 'llm_source' not in st.session_state:
                st.session_state.llm_source = "Ollama"

            llm_sources = ["Ollama", "Groq"] if DOCKER_USE else ["LM Studio", "Ollama", "Groq"]
            st.session_state.llm_source = st.radio("LLM ì†ŒìŠ¤ ì„ íƒ:", llm_sources)

            if st.session_state.llm_source == "Ollama":
                self.set_llm_name(llm_name="Ollama")
                self.setup_ollama(self.app)
            elif st.session_state.llm_source == "Groq":
                self.set_llm_name(llm_name="Groq")
                self.setup_groq(self.app)
            elif not DOCKER_USE and st.session_state.llm_source == "LM Studio":
                self.set_llm_name(llm_name="LM Studio")
                self.setup_lm_studio(self.app)

        except Exception as e:
            st.error(f"LLM ëª¨ë¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    

    def display_chat_interface(self):
        try:
            with self.maincol1:           
                input_container = st.container()
                chat_history = st.container()
                # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
                with input_container:
                    user_question = st.chat_input("Enter your search query", key=st.session_state.chat_input_key)
                    if user_question:
                        st.session_state.messages.append(ChatMessage(role="user", content=user_question))
                        try:
                            asyncio.run(self.app.process_query_async(user_question))
                        except Exception as e:
                            st.error(f"An error occurred: {str(e)}")
                        self.update_chat_history(chat_history)
        except Exception as e:
            error_message = f"display_chat_interface ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            
    def display_chat_messages(self):
        try:
            for message in st.session_state.messages[-8:]:  # ìµœì‹  4ê°œ ë©”ì‹œì§€ë§Œ í‘œì‹œ
                with st.chat_message(message.role):
                    st.markdown(message.content)
        except Exception as e:
            error_message = f"display_chat_messages ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
   
    def update_chat_history(self, chat_history):
        try:
            with chat_history:
                st.session_state.messages = st.session_state.messages[-8:]  # ìµœì‹  4ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
                for message in reversed(st.session_state.messages):
                    if message.role == "user":
                        st.chat_message("user").write(message.content)
                    else:
                        st.chat_message("assistant").write(message.content)
        except Exception as e:
            error_message = f"update_chat_history ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
    

    def display_search_results(self, results):
        try:
            with self.maincol2:            
                if not results:
                    st.write("No results found.")
                else:
                    for i, result in enumerate(results, 1):
                        key = "view_" + str(i)
                        with st.expander(f"Result {i}"):
                            if isinstance(result, dict):
                                st.write(f"**Content:** {result['page_content']}")
                                st.write(f"**Metadata:** {result['metadata']}")
                                st.write(f"**Score:** {result['score']:.1f}")
                                if st.button(f"**File_name:** {result['metadata']['file_name']}", key=key):
                                    self.app.show_pdf(result['metadata']['file_name'])
                            else:
                                st.write(f"**Content:** {result.page_content}")
                                st.write(f"**Metadata:** {result.metadata}")
                                st.write(f"**Score:** {result.metadata.get('score', 'N/A'):.1f}")
                                if st.button(f"**File_name:** {result.metadata.get('file_name', 'Unknown')}", key=key):
                                    self.app.show_pdf(result.metadata.get('file_name'))
        except Exception as e:
            error_message = f"display_search_results ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
                

class RAGChatApp:
    def __init__(self):
        project_root = Path(__file__).parent.parent
        # .env íŒŒì¼ì˜ ê²½ë¡œ ì„¤ì •
        env_path = project_root / '.env'
        load_dotenv(dotenv_path=env_path)
        self.chromadb = ChromaDbManager()
        self.persist_directory = self.chromadb.get_persist_directory;
        self.collection_name = "rag_test"
        self.db_manager = self.chromadb
        self.lm_llm = load_llm()
        self.llm = self.lm_llm
        self.lmmname =""
        self.initialize_session_state()
        self.ui = RAGChatAppUI(self)
        os.environ["POSTHOG_DISABLED"] = "1"
        self.embeddings = load_embeddings()
        self.models = self.load_models_from_env("LM")
        self.groq = GroqManager()
        self.groq_models = self.load_models_from_env("GROQ")
        self.baseurl = "http://localhost:1234/v1"
        
    async def process_query_async(self, query):
        try:
            keyword, parsed_query = self.parse_input(query)
            if (keyword == "f" or st.session_state.apply_filter) and st.session_state.rag_usage == "Rag+LLM":
                await self.process_filtered_query_async(parsed_query)
            else:
                await self.process_regular_query_async(parsed_query)
        except Exception as e:
            error_message = f"process_query_async ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    async def process_filtered_query_async(self, query):
        try:
            if st.session_state.selected_sources:
                docs = await asyncio.to_thread(self.db_manager.get_documents_by_source, 
                                            self.collection_name, st.session_state.selected_sources)
                if docs:
                    await self.generate_response_async(docs, query)
                else:
                    st.write("No documents found for the selected sources.")
            else:
                st.info("ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”!")
        except Exception as e:
            error_message = f"process_filtered_query_async ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    async def process_regular_query_async(self, query):
        try: 
            docs=[]
            if st.session_state.RagUsage == "Rag+LLM":
                with st.spinner('Rag+LLM ë‹µë³€ ì‚¬ì „ ì¤€ë¹„ì¤‘...'):
                    docs = await asyncio.to_thread(self.perform_search, query, self.db_manager, 
                                                self.collection_name, st.session_state.selected_sources)
                if docs:
                    await self.generate_response_async(docs, query)
                    self.ui.display_search_results(docs)
                else:
                    await self.fallback_to_llm_async(query)
            else:
                await self.generate_response_async(None, query)
        except asyncio.CancelledError:
            st.error("ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        except RuntimeError as e:
            st.error(f"ëŸ°íƒ€ì„ ì˜¤ë¥˜: {e}")
        except Exception as e:
            error_message = f"process_regular_query_async ì˜¤ë¥˜ ë°œìƒ: {e}"

    def get_model_name(self):
        return self.lmmname
    
    def set_model_name(self, name):
        self.lmmname = name            
             
    def ollama_generate(self, model_name: str, prompt: str, **kwargs) -> str:
        base_url = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
        url = f"{base_url}/api/generate"
        payload = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
            **kwargs
        }
        
        headers = {
            'Content-Type': 'application/json'
        }

        try:
            response = requests.post(url, json=payload, headers=headers)
            
            response.raise_for_status()
            
           
            try:
                data = response.json()
                return data.get('response', '')
            except json.JSONDecodeError as e:
                raise Exception(f"Ollama API ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")            
        except requests.RequestException as e:
            raise Exception(f"Ollama API HTTP ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            raise Exception(f"Ollama API ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            
     
    async def generate_response_async(self, docs, query):
        try:         
            
            with st.spinner('AI ë‹µë³€ì„ ê¸°ë‹¤ë¦¬ê³  ìˆëŠ”ì¤‘...'):
                if self.ui.llm_name == "Groq":
                    docs_list = [docs] if isinstance(docs, str) else docs
                    response = GroqResponse(self.groq, docs_list, query)
                    content = response['content'] if isinstance(response['content'], str) else str(response['content'])
                    response = content
                elif self.ui.llm_name == "Ollama":
                    # Ollama API ì‚¬ìš©
                    context = "\n".join(doc.page_content for doc in docs) if isinstance(docs, list) else docs
                    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
                    modelname = self.get_model_name()
                    response = self.ollama_generate(model_name=modelname, prompt=prompt)  # model ì¸ì ì œê±°
                    response = response.strip()
                else:
                    chain = load_qa_chain(self.llm, chain_type="stuff", verbose=True)
                    response = await asyncio.to_thread(chain.run, input_documents=docs or "", question=query)
                st.session_state.messages.append(ChatMessage(role="assistant", content=response))
        except Exception as e:
            error_message = f"LLM ì„œë²„ì—°ê²° ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            st.session_state.messages.append(ChatMessage(role="assistant", content=error_message))


    async def fallback_to_llm_async(self, query):
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ LLMì— ì§ˆì˜í•©ë‹ˆë‹¤.")
        await self.generate_response_async("", query)
        
           

    def initialize_session_state(self):
        try:
            if "messages" not in st.session_state:
                st.session_state.messages = []
            if "docs" not in st.session_state:
                st.session_state.docs = None
            if "selected_sources" not in st.session_state:
                st.session_state.selected_sources = []
            if "last_query" not in st.session_state:
                st.session_state.last_query = ""
            if "rag_usage" not in st.session_state:
                st.session_state.rag_usage = "Rag+LLM"
            if "max_docnum" not in st.session_state:
                st.session_state.max_docnum = 3
            if "apply_filter" not in st.session_state:
                st.session_state.apply_filter = False
            if "filtered_sources" not in st.session_state:
                st.session_state.filtered_sources = []
            if "RagUsage" not in st.session_state:    
                st.session_state.RagUsage = "Rag+LLM"
                
                    # ìƒˆë¡œìš´ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
            if "show_search_results" not in st.session_state:
                st.session_state.show_search_results = False
            if "files_processed" not in st.session_state:
                st.session_state.files_processed = False
            if "updated_collections" not in st.session_state:
                st.session_state.updated_collections = self.db_manager.get_list_collections()
            if "need_update" not in st.session_state:
                st.session_state.need_update = False
            if "current_tab" not in st.session_state:
                st.session_state.current_tab = None
            
        except Exception as e:
            error_message = f"initialize_session_state ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
        
    def update_collections(self):
        st.session_state.updated_collections = self.db_manager.get_list_collections()
        st.session_state.need_update = False

    def set_collection_name(self, ragname):
        try:
            self.collection_name = ragname
        except Exception as e:
            error_message = f"set_collection_name ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def set_llm_model(self, modelname, url):
        try:
            self.llm = modelname
            self.baseurl = url
        except Exception as e:
            error_message = f"set_llm_model ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
    
    def list_ollama_models(self):
        try:
            OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')
            response = requests.get(f"{OLLAMA_HOST}/api/tags")
            if response.status_code == 200:
                data = response.json()
                # 'models' í‚¤ê°€ ì¡´ì¬í•˜ê³  ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                if 'models' in data and isinstance(data['models'], list):
                    # ê° ëª¨ë¸ ë°ì´í„°ì—ì„œ 'name' í•„ë“œë§Œ ì¶”ì¶œ
                    return [model['name'] for model in data['models'] if 'name' in model]
                else:
                    return []  # 'models' í‚¤ê°€ ì—†ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            else:
                logger.debug(f"Error: {response.status_code}, {response.text}")
                return []
        except requests.RequestException as e:
            logger.debug(f"Request failed: {e}")
            return []

    def get_ollama_models(self):
        try:
            models = self.list_ollama_models()
            if not models:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
                st.warning("ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return models
        except Exception as e:
            error_message = f"get_ollama_models ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            return []  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    def load_groq(self):
        try:
            project_root = Path(__file__).parent.parent
            env_path = project_root / '.env'
            
            # ë§¤ë²ˆ .env íŒŒì¼ì„ ìƒˆë¡œ ë¡œë“œ
            load_dotenv(dotenv_path=env_path, override=True)
            
            api_key = os.environ.get("GROQ_API_KEY")
            if not api_key:
                st.warning("GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                new_api_key = st.text_input("Groq API Key", type="password")
                if new_api_key:
                    # .env íŒŒì¼ì— API í‚¤ ì €ì¥
                    set_key(env_path, "GROQ_API_KEY", new_api_key)
                    st.success("API í‚¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    # ìƒˆë¡œ ì €ì¥ëœ API í‚¤ë¥¼ ì¦‰ì‹œ ë¡œë“œ
                    load_dotenv(dotenv_path=env_path, override=True)
                    api_key = os.environ.get("GROQ_API_KEY")
                else:
                    st.error("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    return None

            available_models = self.get_groq_models()
            if not available_models:
                st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Groq ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None

            self.ui.set_llm_name(llm_name="Groq")
            
            # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„ íƒ
            selected_model = available_models[0] if isinstance(available_models, list) else available_models
            self.lm_llm.model_name = selected_model
            self.set_llm_model(self.groq, "")

            try:
                groq_llm = ChatGroq(
                    groq_api_key=api_key,
                    model_name=selected_model,
                    temperature=0,
                )
                st.success(f"Groq ëª¨ë¸ '{selected_model}'ì´(ê°€) ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return groq_llm
            except Exception as e:
                st.error(f"Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return None

        except Exception as e:
            st.error(f"load_groq ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    
    
    def load_models_from_env(self,name):
        try:
            models = {}
            for key, value in os.environ.items():
                if name == "LM":
                    if key.startswith('LM_STUDIO_MODEL_'):
                        model_name = key.replace('LM_STUDIO_MODEL_', '')
                        models[model_name] = value
                if name == "GROQ":
                    if key.startswith('GROQ_MODEL_'):
                        model_name = key.replace('GROQ_MODEL_', '')
                        models[model_name] = value            
            return models
        except Exception as e:
            error_message = f"load_models_from_env ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def get_lm_studio_models(self):
        try:
            if not self.models:
                st.error("No LM Studio models defined in .env file.")
                return None

            selected_model = st.selectbox(
                "Select LM Studio Model",
                options=list(self.models.keys()),
                format_func=lambda x: f"{x} ({self.models[x]})"
            )

            if selected_model:
                model_id = self.models[selected_model]
                return model_id
            else:
                return None
        except Exception as e:
            error_message = f"get_llm_studio_modelsì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
    
    def get_groq_models(self):
        try:
            if not self.groq_models:
                st.error("No Groq models defined in .env file.")
                return None

            selected_groqmodel = st.selectbox(
                "Select Groq Model",
                options=list(self.groq_models.keys()),
                format_func=lambda x: f"{x} ({self.groq_models[x]})"
            )

            if selected_groqmodel:
                model_id = self.groq_models[selected_groqmodel]
                return model_id
            else:
                return None
        except Exception as e:
            error_message = f"get_groq_models ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def parse_input(self, input_text):
        try:
            parts = input_text.split(']', 1)
            if len(parts) > 1 and parts[0].startswith('['):
                keyword = parts[0][1:].strip().lower()
                query = parts[1].strip()
                return keyword, query
            else:
                return None, input_text.strip()
        except Exception as e:
            error_message = f"parse_input ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)


    def perform_search(self, query, db_manager, collection_name, selected_sources=None):
        try:
            raw_results = db_manager.search_collection(collection_name, query, n_results=FILLTERED_DOC_NUMBER)        
            if not raw_results:
                return []
            docs = []
            for result in raw_results:
                if not isinstance(result['page_content'], str):
                    result['page_content'] = str(result['page_content'])
                
                doc = Document(
                    # ì›ë³¸ textë¥¼ summarizeë¥¼ í•´ì„œ ë³´ë‚´ëŠ” ë°©ë²•-LLM ì†ë„ê°œì„  í…ŒìŠ¤íŠ¸
                    page_content=TextSummarizer.summarize(result['page_content'], 5),
                    # ì›ë³¸ textë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•
                    # page_content=result['page_content'],
                    metadata={"score": result['score'], **result['metadata']}
                )            
                docs.append(doc)
                
            filtered_docs = [doc for doc in docs]
            
            if selected_sources:
                filtered_docs = [doc for doc in filtered_docs if doc.metadata.get('source', 'Unknown') in selected_sources]
            
            return filtered_docs
        except Exception as e:
            error_message = f"perform_search ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)


    def process_query(self, query):
        try:
            keyword, parsed_query = self.parse_input(query)
            if (keyword == "f" or st.session_state.apply_filter) and st.session_state.rag_usage == "Rag+LLM":
                self.process_filtered_query(parsed_query)
            else:
                self.process_regular_query(parsed_query)
        except Exception as e:
            error_message = f"process_query ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def process_filtered_query(self, query):
        try:
            if st.session_state.selected_sources:
                docs = self.db_manager.get_documents_by_source(self.collection_name, st.session_state.selected_sources)
                if docs:
                    self.generate_response(docs, query)
                else:
                    st.write("No documents found for the selected sources.")
            else:
                st.info("ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”!")
        except Exception as e:
            error_message = f"process_filtered_queryì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def process_regular_query(self, query):
        try:
            asyncio.run(self.process_regular_query_async(query))
            if st.session_state.RagUsage == "Rag+LLM":
                with st.spinner('Rag+LLM ë‹µë³€ ì‚¬ì „ ì¤€ë¹„ì¤‘...'):
                    docs = self.perform_search(query, self.db_manager, self.collection_name, st.session_state.selected_sources)                
                if docs:
                    self.generate_response(docs, query)                
                    self.ui.display_search_results(docs)
                else:
                    self.fallback_to_llm(query)
            else:
                self.generate_response(None, query)            
        except Exception as e:
            error_message = f"process_regular_queryì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            
    def generate_response(self, docs, query):
        try:
            # Create the prompt template
            prompt_template = """
            System: You are an AI assistant that answers questions using only the provided information. Do not include any content not present in the given information. If the information is insufficient, say "I cannot answer with the given information."

            Context: {context}

            Human: {question}

            AI: """

            # Prepare the context from the retrieved documents
            context = "\n".join([doc.page_content for doc in docs])

            # Create the prompt
            prompt = prompt_template.format(context=context, question=query)

            # Load the QA chain
            chain = load_qa_chain(self.llm, chain_type="stuff", verbose=True)

            with st.spinner('AI ë‹µë³€ì„ ê¸°ë‹¤ë¦¬ê³  ìˆëŠ”ì¤‘...'):
                # Run the chain with the prepared prompt
                response = chain.run(input_documents=docs or "", question=prompt)
                st.session_state.messages.append(ChatMessage(role="assistant", content=response))
        except Exception as e:
            error_message = f"LLM ì„œë²„ì—°ê²° ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            st.session_state.messages.append(ChatMessage(role="assistant", content=error_message))            

    
    def fallback_to_llm(self, query):
        try:
            st.info("ì„ íƒí•œ ì†ŒìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ LLMì— ì§ˆì˜í•©ë‹ˆë‹¤.")
            self.generate_response(None, query)
        except Exception as e:
            error_message = f"fallback_to_llm ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def show_pdf(self, file_path):
        try:
            file_path = os.path.join(self.persist_directory, file_path)
            with open(file_path, "rb") as f:
                base64_pdf = base64.b64encode(f.read()).decode('utf-8')
            pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="800" height="800" type="application/pdf"></iframe>'
            st.markdown(pdf_display, unsafe_allow_html=True)
        except Exception as e:
            error_message = f"show_pdf ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)

    def run(self):
        try:
            self.ui.setup_main_ui()
            with st.sidebar:
                self.ui.setup_sidebar()
            self.ui.display_chat_interface()
        except Exception as e:
            error_message = f"run ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
        
if __name__ == '__main__':
    if platform.system() != 'Windows':
        logger.debug("ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Windowsì—ì„œë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    app = RAGChatApp()
    app.run()
    